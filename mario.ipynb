{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\AndyW\\OneDrive\\Documents\\GitHub\\rl-mario-learning\\envs\\mario\\lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: [WinError 127] The specified procedure could not be found\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import transforms as T\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from collections import deque\n",
    "import random, time, datetime, os, copy\n",
    "import numpy as np\n",
    "\n",
    "# Gym is an OpenAI toolkit for RL\n",
    "import gym\n",
    "from gym.spaces import Box\n",
    "from gym.wrappers import FrameStack\n",
    "\n",
    "# NES Emulator for OpenAI Gym\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "\n",
    "# Super Mario environment for OpenAI Gym\n",
    "import gym_super_mario_bros\n",
    "\n",
    "# imageio is used to save the training progress as a gif\n",
    "import imageio\n",
    "\n",
    "# plotly is used to view the training progress\n",
    "import plotly.express as px\n",
    "\n",
    "# kaleido is used to export the plotly figure as a png\n",
    "import kaleido\n",
    "\n",
    "# pyvirtualdisplay is used to create a virtual display to watch the agent play\n",
    "from pyvirtualdisplay import Display\n",
    "\n",
    "# progress bars:\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\AndyW\\OneDrive\\Documents\\GitHub\\rl-mario-learning\\envs\\mario\\lib\\site-packages\\gym\\envs\\registration.py:555: UserWarning: \u001b[33mWARN: The environment SuperMarioBros-v0 is out of date. You should consider upgrading to version `v3`.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<TimeLimit<OrderEnforcing<PassiveEnvChecker<SuperMarioBrosEnv<SuperMarioBros-v0>>>>>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smbe = gym_super_mario_bros.SuperMarioBrosEnv()\n",
    "gym_super_mario_bros.make('SuperMarioBros-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                 SuperMarioBrosEnv\n",
       "1     SuperMarioBrosRandomStagesEnv\n",
       "14                             make\n",
       "15                          smb_env\n",
       "16            smb_random_stages_env\n",
       "dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "s = pd.Series(dir(gym_super_mario_bros))\n",
    "s[~s.str.contains(\"__\") & ~s.str.startswith(\"_\")]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL Definitions\n",
    "- **Environment** The world that an agent interacts with and learns from.\n",
    "- **Action** $a$ : How the Agent responds to the Environment. The set of all possible Actions is called *action-space*.\n",
    "- **State** $s$ : The current characteristic of the Environment. The set of all possible States the Environment can be in is called *state-space*.\n",
    "- **Reward** $r$: Reward is the key feedback from Environment to Agent. It is what drives the Agent to learn and to change its future action. An aggregation of rewards over multiple time steps is called **Return**\n",
    "- **Optimal Action-Value function** $Q^*(s,a)$ : Gives the expected return if you start in state $s$, take an arbitrary action $a$, and then for each future time step take the action that maximizes returns. $Q$ can be said to stand for the “quality” of the action in a state. We try to approximate this function.\n",
    "- **Q-Value** $Q(s,a)$ : The expected return if you start in state $s$, take action $a$, and then for each future time step take the action that maximizes returns. $Q$ can be said to stand for the “quality” of the action in a state. We try to approximate this function."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment\n",
    "\n",
    "### Initialize Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "next_state.shape: (240, 256, 3),\n",
      " reward: 0.0,\n",
      " done: False\n",
      "\n",
      "coins: 0\n",
      "flag_get: False\n",
      "life: 2\n",
      "score: 0\n",
      "stage: 1\n",
      "status: small\n",
      "time: 400\n",
      "world: 1\n",
      "x_pos: 40\n",
      "y_pos: 79\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "from gym.utils import passive_env_checker\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"gym.envs.registration\")\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning, module=passive_env_checker.__name__)\n",
    "warnings.filterwarnings(\"ignore\", category=np.VisibleDeprecationWarning)\n",
    "\n",
    "# version of super mario bros\n",
    "smb_version = 'SuperMarioBros-1-1-v3'\n",
    "\n",
    "# initialize env based on gym version\n",
    "if gym.__version__ < \"0.26\":\n",
    "    env = gym_super_mario_bros.make(smb_version\n",
    "                                    , new_step_api=True)\n",
    "else:\n",
    "    env = gym_super_mario_bros.make(smb_version\n",
    "                                    , render_mode='human'\n",
    "                                    , apply_api_compatibility=True\n",
    "                                    # , mode='rgb_array'\n",
    "                                    )\n",
    "\n",
    "# limit the possible actions to:\n",
    "#   0. walk right\n",
    "#   1. jump right\n",
    "env = JoypadSpace(env,\n",
    "                  [\n",
    "                    # 0. walk right\n",
    "                    [\"right\"]\n",
    "                    \n",
    "                    # 1. jump right\n",
    "                    , [\"right\", \"A\"]\n",
    "                    ])\n",
    "\n",
    "env.metadata['render_modes'] = ['human', 'rgb_array']\n",
    "\n",
    "env.reset()\n",
    "\n",
    "next_state, reward, done, trunc, info = env.step(action=0)\n",
    "print(f\"\\nnext_state.shape: {next_state.shape},\\n reward: {reward},\\n done: {done}\\n\")\n",
    "\n",
    "for k, v in info.items():\n",
    "    print(f\"{k}: {v}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess environment\n",
    "\n",
    "- mario doesn't need to know *everything* about the environment\n",
    "- only needs to know what it needs to know to make a decision\n",
    "\n",
    "preprocessing the environment is a way to reduce the state space, and make it easier for the agent to learn\n",
    "\n",
    "#### preprocessing steps:\n",
    "Use a bunch of `Wrappers` from `gym.wrappers` to preprocess the environment\n",
    "1. convert to grayscale `GreyScaleObservation` \n",
    "    - reduces the number of channels from 3 to 1\n",
    "    - new `next_state.shape` is `(1, 240, 256)`\n",
    "2. downsample/resize to a square image: `ResizeObservation` \n",
    "    - reduces the size of the image\n",
    "    - new `next_state.shape` is `(1, 84, 84)`\n",
    "3. consecutive frames don't change much, so we can skip every nth frame with `SkipFrame` \n",
    "    - reduces the number of frames\n",
    "    - new `next_state.shape` is `(1, 84, 84)`, but the total number of frames is reduced by a factor of `n`\n",
    "    - `SkipFrame` inherits from `gym.Wrapper`, which is a wrapper for the environment\n",
    "4. stack the frames to get a short history of the environment: `FrameStack` \n",
    "    - squashes the frames into a single array\n",
    "    - we can ID if mario is moving left or right or jumping by looking at the last few frames"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Convert to Grayscale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GrayScaleObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        obs_shape = self.observation_space.shape[:2]\n",
    "        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
    "\n",
    "    def permute_orientation(self, observation):\n",
    "        # permute [H, W, C] array to [C, H, W] tensor\n",
    "        observation = np.transpose(observation, (2, 0, 1))\n",
    "        observation = torch.tensor(observation.copy(), dtype=torch.float)\n",
    "        return observation\n",
    "\n",
    "    def observation(self, observation):\n",
    "        observation = self.permute_orientation(observation)\n",
    "        transform = T.Grayscale()\n",
    "        observation = transform(observation)\n",
    "        return observation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Downsample/Resize to a Square Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResizeObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env, shape):\n",
    "        super().__init__(env)\n",
    "        if isinstance(shape, int):\n",
    "            self.shape = (shape, shape)\n",
    "        else:\n",
    "            self.shape = tuple(shape)\n",
    "\n",
    "        obs_shape = self.shape + self.observation_space.shape[2:]\n",
    "        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        transforms = T.Compose(\n",
    "            [T.Resize(self.shape), T.Normalize(0, 255)]\n",
    "        )\n",
    "        observation = transforms(observation).squeeze(0)\n",
    "        return observation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Skip Every Nth Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipFrame(gym.Wrapper):\n",
    "    def __init__(self, env, skip):\n",
    "        \"\"\"Return only every `skip`-th frame\"\"\"\n",
    "        super().__init__(env)\n",
    "        self._skip = skip\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Repeat action, and sum reward\"\"\"\n",
    "        total_reward = 0.0\n",
    "        for i in range(self._skip):\n",
    "            # Accumulate reward and repeat the same action\n",
    "            obs, reward, done, trunk, info = self.env.step(action)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        return obs, total_reward, done, trunk, info"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Stack the Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Wrappers to environment\n",
    "env = SkipFrame(env, skip=4)\n",
    "env = GrayScaleObservation(env)\n",
    "env = ResizeObservation(env, shape=84)\n",
    "if gym.__version__ < '0.26':\n",
    "    env = FrameStack(env, num_stack=4, new_step_api=True)\n",
    "else:\n",
    "    env = FrameStack(env, num_stack=4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent\n",
    "\n",
    "Create a `Mario` class to represent the agent in the game. `Mario` should be able to:\n",
    "1. take an action according to the optimal action policy based on the current state of the environment\n",
    "2. remember experiences, where an experience = (current state, current action, reward, next state)\n",
    "   -  `Mario` caches and later recalls experiences to update the action policy\n",
    "3. learn a better action policy based on the experiences it has had\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA: False\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8a2252423b34db399e94cbf80ea9cc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Episodes:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\AndyW\\OneDrive\\Documents\\GitHub\\rl-mario-learning\\envs\\mario\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:289: UserWarning: \u001b[33mWARN: No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 - Step 534 - Epsilon 0.9998665088940238 - Mean Reward 1264.0 - Mean Length 534.0 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 8.075 - Time 2023-04-23T08:21:23\n"
     ]
    }
   ],
   "source": [
    "# Create a virtual display if on a linux server\n",
    "if os.name == \"posix\":\n",
    "    display = Display(visible=0, size=(1400, 900))\n",
    "    display.start()\n",
    "\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(f\"Using CUDA: {use_cuda}\")\n",
    "print()\n",
    "\n",
    "save_dir = Path(\"checkpoints\") / datetime.datetime.now().strftime(\"%Y-%m-%dT%H-%M-%S\")\n",
    "save_dir.mkdir(parents=True)\n",
    "\n",
    "mario = Mario(state_dim=(4, 84, 84), action_dim=env.action_space.n, save_dir=save_dir)\n",
    "\n",
    "logger = MetricLogger(save_dir)\n",
    "\n",
    "episodes = 10\n",
    "# Initialize a list to store the frames for each episode\n",
    "frames = []\n",
    "\n",
    "for e in tqdm(range(episodes), desc=\"Episodes\"):\n",
    "    state = env.reset()\n",
    "    # Clear the frames list for each episode\n",
    "    episode_frames = []\n",
    "\n",
    "    while True:\n",
    "        action = mario.act(state)\n",
    "        next_state, reward, done, trunc, info = env.step(action)\n",
    "\n",
    "        # Render the environment and save the frame\n",
    "        frame = env.render()\n",
    "        episode_frames.append(frame)\n",
    "\n",
    "        mario.cache(state, next_state, action, reward, done)\n",
    "        q, loss = mario.learn()\n",
    "        logger.log_step(reward, loss, q)\n",
    "        state = next_state\n",
    "\n",
    "        if done or info[\"flag_get\"]:\n",
    "            break\n",
    "\n",
    "    print(f\"Episode {e} - Logging Episode\")\n",
    "    logger.log_episode()\n",
    "    if e % 20 == 0:\n",
    "        print(f\"Episode {e} - Saving Model\")\n",
    "        logger.record(episode=e, epsilon=mario.exploration_rate, step=mario.curr_step)\n",
    "\n",
    "    # Add the episode frames to the main frames list\n",
    "    frames.append(episode_frames)\n",
    "\n",
    "# Save the frames as a video\n",
    "print(\"Saving video\")\n",
    "imageio.mimsave('mario_gameplay.mp4', [frame for episode_frames in frames for frame in episode_frames], fps=30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
